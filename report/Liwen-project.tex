\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[]{hyperref}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Exploring Convolutional Neural Networks for Image Classification Problem \\
\author{\IEEEauthorblockN{Liwen Dai}
\IEEEauthorblockA{
\textit{20552153,}
L7DAI@uwaterloo.ca}}
}

\maketitle

\begin{abstract}
In this project, I have built and trained neural networks to solve the image classification problem on the CIFAR-10 dataset. First I reimplemented a model provided by Tensorflow with two convolutional layers as a baseline. Next I reshaped the network by stacking multiple convolutional layers. I also attempted to make the network deeper by adding network-in-network layers and using fractional max pooling. My best result beat the baseline with 86\% accuracy on the test set as well as a much faster speed of convergence.\\   
\end{abstract}

\begin{IEEEkeywords}
image classification, convolution, neural networks, deep learning, artificial intelligence
\end{IEEEkeywords}

\section{Introduction}
CIFAR-10 and CIFAR-100 are two famous datasets to benchmark image classification algorithms \cite{cifar}. The CIFAR-10 dataset consists of 60000 32x32 images in 10 classes, with 5000 training images and 1000 test images each class. The CIFAR-100 datset is similar to CIFAR-10, but is has 100 classes containing 600 images each. An incomplete leaderboard on these two datasets has been collected by \cite{leaderboard}.

Neural networks, particularly, different variations of convolutional networks, have been proved the most effective methods for the task of image classfication. As in \cite{leaderboard}, they have occupied all the top results. But working real-world images of medium or high resolution requires very large neural networks, containing more than millions of parameters, which can be very slow to train\cite{conv}. For this reason many researchers choose to evaluate their methods on these tiny image data sets before scaling. 

However, even with the complexity controlled by limiting the task on tiny images, the state-of-art results still rely on networks of considerable size\cite{frac}. I found it nearly impossible to replicate these models on my laptop, simply because the GPU would run out of memory or it would take too many days, even weeks to train. Considering this fact, the question has arised: what can we achieve if we perform deep learning tasks with limited computing resource? A very intuitive idea is to shrink a large neural network simply by reducing the number of parameters. Another idea is to limit the training to some acceptable time. 

Based on the question above, I reimplemented a convolutional network provided by Tensorflow\cite{tut} as a baseline; then I built deeper networks using multiple techniques, including stacking convolutional layers, adding network-in-network layers and fractional max pooling.

After applying different combinations of techniques and parameter settings, my best model beat the baseline and achieved 86\% test accuracy on the CIFAR-10 data set. Besides that, my model also had a much faster training speed to reach convergence.

I concluded that although networks with shrinked size cannot reach the state-of-art results on CIFAR-10 \cite{leaderboard}, there's still room for improvement. I also discovered that techniques such as dropout may not be as effective in small networks as they are in the larger counterparts. I expect these results and discoveries to be helpful when tackling more complicated problems with limited computing power.

\section{Related Work}
Convolutional neural network has been a very big jump for image classification and object recognition tasks, since introduced as Alexnet\cite{conv}. Alexnet won the ImageNet LSVRC-2010 completition with top-1 error rate of 37.5\% and top-5 error rate of 17.0\%, while the second best model had only reached 45.7\% and 25.7\%\cite{conv}. This neural network had 60 million parameters and 650,000 neurons, consisting of five convolutional layers, with max-pooling layers, dropout, followed by three fully-connected layers and a final softmax output layer\cite{conv}.

In the recent few years, a lot of researchers have applied various techniques to improve the performance of convolutional networks, and many have benchmarked their models on the tiny image data sets, CIFAR-10 and CIFAR-100. The ``Network in Network'' structure was introduce to enhance the model discriminablility for local receptive fields, which achieved 91.2\% and 64.3\% accuracy on CIFAR-10 and CIFAR-100, respectively\cite{nin}. Spatially-sparse convolutional neural networks improved this accuracy to 93.72\% and 75.7\%\cite{sparse}. Applying fractional max pooling pushed the result on CIFAR-10 further to 96.53\%\cite{fmp}. The deep residual network introduced by Mirosoft reached 93.57\% on CIFAR-10\cite{res}; and its ``wide'' variance even reached 96.62\% and 81.7\%, with a much faster training speed\cite{wide}.

It is worth mentioning that nowdays convolutional networks have grown deeper and deeper. Alexnet had 5 convolutional layers and was titled ``Deep convolutional Neural Networks'' when published in 2012\cite{res}. The spatially sparse net with fractional max pooling benchmarked on CIFAR-100 with 14 convolutional layers\cite{fmp}. The deep residual net was even implemented with 1000 layers on CIFAR-10\cite{res}. Luckily, going deeper is not always necessary: the wide but ``shallow'' residual nets reached the top accuracy with 28 convolutional layers\cite{wide} 

\section{Methodology}
\subsection*{Convolutional Layers}


\subsection{Units}
\begin{itemize}
\item blank
\item blank
\end{itemize}

\subsection{Equations}
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}
blank

\subsection{\LaTeX-Specific Advice}

placeholder


\subsection{Authors and Affiliations}
placeholder

\subsection{Identify the Headings}
placeholder

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} placeholder

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

placeholder

\section*{Acknowledgment}

placeholder



\begin{thebibliography}{00}
\bibitem{cifar} Krizhevsky, Alex, and Geoffrey Hinton. ``Learning multiple layers of features from tiny images.'' (2009).
\bibitem{leaderboard} Rodrigo Benenson. ``Classification datasets results.'' \url{http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html}
\bibitem{conv}Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ``Imagenet classification with deep convolutional neural networks.'' Advances in neural information processing systems. 2012.
\bibitem{frac} Graham, Benjamin. ``Fractional max-pooling.'' arXiv preprint arXiv:1412.6071 (2014).
\bibitem{tut} ``Convolutional Neural Networks.'' \url{ https://www.tensorflow.org/tutorials/deep_cnn}
\bibitem{nin} Lin, Min, Qiang Chen, and Shuicheng Yan. ``Network in network.'' arXiv preprint arXiv:1312.4400 (2013).
\bibitem{sparse} Graham, Benjamin. ``Spatially-sparse convolutional neural networks.'' arXiv preprint arXiv:1409.6070 (2014).
\bibitem{fmp} Graham, Benjamin. ``Fractional max-pooling.'' arXiv preprint arXiv:1412.6071 (2014).
\bibitem{res} He, Kaiming, et al. ``Deep residual learning for image recognition.'' Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
\bibitem{wide} Zagoruyko, Sergey, and Nikos Komodakis. ``Wide residual networks.'' arXiv preprint arXiv:1605.07146 (2016).
\end{thebibliography}

\end{document}
